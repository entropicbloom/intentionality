{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from datasets import OneLayerDataset, OneLayerDataModule\n",
    "from set_transformer import SetTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning_model import LightningModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'FullyConnected'\n",
    "dataset_type = 'MNISTDataModule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(OneLayerDataset(model_type, dataset_type, 2), batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = SetTransformer(dim_input=51, num_outputs=10, dim_output=1, num_inds=32, dim_hidden=32, num_heads=4, ln=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(pytorch_model, learning_rate=0.001, num_classes=10)\n",
    "data_module = OneLayerDataModule(model_type, dataset_type, layer_idx=2, input_dim=51, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | SetTransformer     | 33.8 K\n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | valid_acc | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "33.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.8 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/34 [00:00<?, ?it/s, v_num=22, valid_acc=0.082]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x131d29080>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flassig/miniconda3/envs/intentionality/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 34/34 [00:19<00:00,  1.71it/s, v_num=22, valid_acc=0.082]"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=1, mode=\"max\", monitor=\"valid_acc\"\n",
    "        )  # save top 1 model\n",
    "    ]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n",
    "    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n",
    "    logger=logger,\n",
    "    deterministic=False,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.fit(model=lightning_model, datamodule=data_module)\n",
    "\n",
    "runtime = (time.time() - start_time) / 60\n",
    "print(f\"Training took {runtime:.2f} min in total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m data, label \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(pytorch_model(data)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/intentionality/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/projects/intentionality/decoder/set_transformer.py:47\u001b[0m, in \u001b[0;36mSetTransformer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> 47\u001b[0m     output \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(X))\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mview(output\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/intentionality/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/intentionality/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data, label in loader:\n",
    "    print(pytorch_model(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intentionality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
